from damicore import damicore
from time import process_time
from time import time
import pandas as pd
import argparse
import shutil
import json
import sys
import os

def get_args_to_damicore(
        compressor:str, 
        output_cluster_path:str, 
        output_time_path:str = None, 
        output_ncd_path:str = None,
        level:int = None, 
        order:int = None,
        memory:int = None,
        serial:bool = False,
        verbose:bool = False
    ):

    args = []

    args.extend(["--compressor", compressor.split('_')[0]])

    if level:
        args.extend(["--level", str(level)])

    if order:
        args.extend(["--model-order", str(order)])

    if memory:
        args.extend(["--memory", str(memory)])
    
    try:
        if compressor.split('_')[1] == 'lossy':
            args.extend(['--lossy', str(1)])
        else:
            args.extend(['--lossy', str(0)])
    except:
        pass

    args.append("--verbose" if verbose else "--no-verbose")

    args.append("--serial" if serial else "--parallel")

    if output_ncd_path:
        args.extend(["--ncd-output", output_ncd_path])

    # args.extend(["--json-time", output_time_path])

    args.extend(["--output", output_cluster_path, "../sample_data"])

    return args

def execute_damicore(
    compressor:str, 
    type:str,
    output_cluster_dir:str, 
    output_time_dir:str = None, 
    output_ncd_dir:str = None,
    level:int = None, 
    order:int = None,
    memory:int = None,
    serial:bool = False,
    verbose:bool = False,
    file_number:str = "0"
):
    data = []

    # Define o diret√≥rio para os clusters.
    cluster_dir = f"{output_cluster_dir}/{compressor}"
    cluster_path = f"{cluster_dir}/{compressor}_{type}_{file_number}.out"
    
    if not os.path.exists(cluster_dir):
        os.makedirs(cluster_dir)

    # Verifica se h√° output para os dados da NCD.
    if output_ncd_dir:
        ncd_dir = f"{output_ncd_dir}/{compressor}"
        ncd_path = f"{ncd_dir}/{compressor}_{type}_{file_number}.out"
        if not os.path.exists(ncd_dir):
            os.makedirs(ncd_dir)
    else:
        ncd_path = None

    # Obtem os argumentos para a DAMICORE
    args = get_args_to_damicore(
        compressor = compressor,
        output_cluster_path = cluster_path,
        output_ncd_path = ncd_path,
        level = level,
        order = order,
        memory = memory,
        serial = serial,
        verbose = verbose
    )

    # print(args)

    e0 = time()
    p0 = process_time()
    damicore.main(args)
    p = process_time()
    e = time()

    data.append({
        "compressor": compressor, 
        "number": file_number,
        "type": type, 
        "time": e-e0, 
        "process_time": p-p0,
        "output": cluster_path
    })

    return data

def limpa_dir(output_dir:str):
    # Limpa o diret√≥rio de sa√≠da
    if os.path.exists(output_dir):
        # Remove todos os arquivos e subdiret√≥rios dentro de output_dir
        for item in os.listdir(output_dir):
            item_path = os.path.join(output_dir, item)
            if os.path.isfile(item_path) or os.path.islink(item_path):
                os.unlink(item_path)  # Remove arquivos ou links simb√≥licos
            elif os.path.isdir(item_path):
                shutil.rmtree(item_path)  # Remove diret√≥rios recursivamente
    else:
        # Cria o diret√≥rio de sa√≠da se ele n√£o existir
        os.makedirs(output_dir)

    # Aqui voc√™ pode adicionar o restante da l√≥gica da fun√ß√£o
    print(f"[CLEANING] O diret√≥rio {output_dir} foi limpo.")

def copia_arquivo(origem: str, destino: str):
    """
    Copia um √∫nico arquivo para um diret√≥rio de destino.

    Par√¢metros:
    -----------
    origem : str
        Caminho absoluto ou relativo do arquivo de origem a ser copiado.

    destino : str
        Caminho do diret√≥rio de destino onde o arquivo ser√° copiado.

    Comportamento:
    --------------
    - O arquivo ser√° copiado com o mesmo nome no diret√≥rio de destino.
    - A fun√ß√£o preserva metadados (data de modifica√ß√£o, permiss√µes, etc).
    - Se o destino n√£o existir ou n√£o for um diret√≥rio, uma exce√ß√£o ser√° lan√ßada.
    """
    # Verifica se o caminho de origem √© um arquivo existente
    if not os.path.isfile(origem):
        raise FileNotFoundError(f"O arquivo de origem '{origem}' n√£o existe.")

    # Verifica se o destino √© um diret√≥rio v√°lido
    if not os.path.isdir(destino):
        raise NotADirectoryError(f"O destino '{destino}' n√£o √© um diret√≥rio v√°lido.")

    # Obt√©m o nome do arquivo
    nome_arquivo = os.path.basename(origem)

    # Define o caminho completo de destino
    destino_completo = os.path.join(destino, nome_arquivo)

    # Copia o arquivo
    shutil.copy2(origem, destino_completo)

def copia_arquivos(origem: str, destino: str, selection:str = None):
    if os.path.exists(origem):
        for item in os.listdir(origem):
            
            origem_item = os.path.join(origem, item)
            destino_item = os.path.join(destino, item)

            if not selection:
                if os.path.isfile(origem_item):
                    shutil.copy2(origem_item, destino_item)  # Copia o arquivo preservando metadados
                elif os.path.isdir(origem_item):
                    shutil.copytree(origem_item, destino_item)  # Copia diret√≥rios recursivamente
            else:
                if selection in item:
                    print(f"Copiando {item}...")
                    if os.path.isfile(origem_item):
                        shutil.copy2(origem_item, destino_item)  # Copia o arquivo preservando metadados
                    elif os.path.isdir(origem_item):
                        shutil.copytree(origem_item, destino_item)  # Copia diret√≥rios recursivamente
        print(f"[COPY] Arquivos de {origem} foram copiados para {destino}")

def copia_samples(main_dir:str, var_dir:str, output_dir:str, selection:str = None):
    limpa_dir(output_dir)

    # Copia os arquivos de main_dir para output_dir
    copia_arquivos(main_dir, output_dir, selection=selection)

    # Copia os arquivos de var_dir para output_dir
    copia_arquivos(var_dir, output_dir, selection=selection)

def processing_time_to_json(json_data, output:str="../data.json"):
    if not os.path.exists(os.path.dirname(output)):
        os.makedirs(os.path.dirname(output))

    try:
        with open(output, "r", encoding="utf-8") as json_file:
            read_data = json.load(json_file)
            
    except (FileNotFoundError, json.JSONDecodeError) as e:
        read_data = []

    read_data.extend(json_data)

    with open(output, "w", encoding="utf-8") as json_file:
        json.dump(read_data, json_file, indent=4, ensure_ascii=False)

def print_progress_bar(current:int, min:int = 0, max:int = 100, max_relative=40):
    if current < min or current > max:
        return
    
    progress = (current/(max - min))
    progress_relative = int(progress * max_relative)

    print("\033[32m", end='')
    for i in range (0, progress_relative):
        print('üü©', end='')
    for i in range (progress_relative + 1, max_relative):
        print('üü•', end='')
    print(f" {progress * 100:.1f}%\033[0m")
 
def verify_singular_group(file: str):
    df = pd.read_csv(file, header=None)
    df.columns = ['file_name', 'group']
    # Extrai partes da string com regex direto da coluna 'file_name'
    df[['load', 'type', 'file']] = df["file_name"].str.extract(
    r'^(?P<load>[^_]+)_.*?_code_(?P<type>[A-Za-z\-]+)_py_(?P<file>\d+)_csv$'
)
    df['file'] = df['file'].astype(int)
    df['group'] = df['group'].astype(int)
    df = df[['load', 'type', 'file', 'group']]

    filtros = df.groupby("group")["type"].apply(lambda x: (x != "CONTROL").all())
    exclusive_groups = filtros[filtros].index.tolist()
    
    return len(exclusive_groups) > 0
if __name__ == '__main__':

    # Configura os argumentos para o script.
    parser = argparse.ArgumentParser(description="Executa o DAMICORE com diversos par√¢metros configur√°veis.")

    # Argumento posicional obrigat√≥rio
    parser.add_argument("compressor", help="Nome do compressor")

    # Flags opcionais
    parser.add_argument("--serial", action="store_true", help="Executar em modo serial")
    parser.add_argument("--verbose", action="store_true", help="Exibir sa√≠das detalhadas")

    # Argumentos opcionais com valores padr√£o
    parser.add_argument("--level", type=int, default=9, help="N√≠vel de compress√£o (padr√£o: 9)")
    parser.add_argument("--order", type=int, default=16, help="Ordem do modelo (padr√£o: 16)")
    parser.add_argument("--memory", type=int, default=16, help="Tamanho da mem√≥ria (padr√£o: 16)")
    parser.add_argument("--reps", type=int, default=100, help="Tamanho da mem√≥ria (padr√£o: 16)")

    args = parser.parse_args()

    # Configura√ß√£o de caminhos e tipo fict√≠cio (ajuste para o seu contexto)
    compressor = args.compressor
    serial_flag = args.serial
    verbose_flag = args.verbose
    level_value = args.level
    order_value = args.order
    memory_value = args.memory
    repetition = args.reps


    default_dir = "../../../data/profiles/original_data"
    incremental_output = "../../../results"

    selected_load = ['CANCER-PHON']
    # selected_type = ['API', 'MEMORY', 'PROCESS']
    # selected_type = ['API', 'CONC', 'LOGIC', 'MEMORY', 'MODEL', 'PROCESS', 'TRAIN', 'ANOTHER-CONTROL']
    selected_type = ['ANOTHER-CONTROL']
    compressors = ["zlib", "gzip", "bz2", "ppmd", "webp_lossless", "png", "jp2_lossless", "entropy", "webp_lossy", "jp2_lossy", "heif"]

    # Para cada load.
    for load in os.listdir(default_dir):
        if not load in selected_load: # Se n√£o est√° dentro dos loads selecionados.
            continue
        
        load_dir = f"{default_dir}/{load}"
        if not os.path.isdir(load_dir): # Verifica se n√£o √© um diret√≥rio.
            continue
        
        output_load_path = os.path.join("../clusters_wtime/", load) # Obt√©m o diret√≥rio de sa√≠da de clusters.
        control_dir = f"{load_dir}/CONTROL" # Obt√©m o diret√≥rio de controle da carga.

        os.makedirs(f"{incremental_output}/{load}", exist_ok=True)

        # Para cada tipo dentro de load.
        for type in os.listdir(load_dir): 
            if not type in selected_type: # Se n√£o est√° dentro dos tipos selecionados.
                continue
            type_dir = os.path.join(load_dir, type)
            output_path = os.path.join(output_load_path, type)

            profile_paths = [f for f in os.listdir(type_dir) if os.path.isfile(os.path.join(type_dir, f))]

            for rep in range(repetition):
                limpa_dir("../sample_data")
                copia_arquivos(control_dir, "../sample_data") # Copia os arquivos CONTROL para sample_data

                print(f"\n\033[33mLOAD: {load}")
                print(f"TYPE: {type}")
                print(f"COMPRESSOR: {compressor}\n")

                total_time = 0
                for i in range(-1, len(profile_paths)):
                    if i != -1:
                        path = f"{type_dir}/{profile_paths[i]}"
                        copia_arquivo(path, "../sample_data")
                        print(f"\033[36mADICIONANDO {os.path.basename(path)} AOS GRUPOS\033[0m")
                    else:
                        print("\033[36mCONFIGURANDO AGRUPAMENTO DE CONTROLE...\033[0m")
                    if compressor in compressors:                    
                        data = execute_damicore(
                            compressor = compressor,
                            type = type,
                            output_cluster_dir = output_path,
                            level = 9,
                            order = 16,
                            memory = 16,
                            verbose=False,
                            serial=serial_flag,
                            file_number = i if i != -1 else "default"
                        )
                        total_time += data[0]['time']
                        processing_time_to_json(data, f"../times_wtime/{load}.json")
                        is_singular = verify_singular_group(data[0]["output"])
                        if(is_singular):
                            break
                processing_time_to_json(
                    [{
                        "load": load,
                        "type": type,
                        "compressor": compressor,
                        "iteration": i if is_singular else -1,
                        "time": total_time,
                        "repetition": rep
                    }],
                    f"{incremental_output}/{load}/{type}")
                # TRANSFORMAR OS .OUT EM JSON

                
